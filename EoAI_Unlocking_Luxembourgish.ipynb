{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZLSLU/EoAI/blob/main/EoAI_Unlocking_Luxembourgish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUpX-erilhYS"
      },
      "source": [
        "# Elements of AI : Unlocking Luxembourgish\n",
        "\n",
        "Welcome to Unlocking Luxembourgish. In this session you'll work with a temporary public API (at **34.90.113.55:5000**) to experiment with our Luxembourgish AI models for Speech-to-Text (STT) and Text-to-Speech (TTS).\n",
        "\n",
        "The notebook is divided into two parts:\n",
        "\n",
        "**Part I: STT Exercises** – You'll test various STT parameters (such as VAD, disfluency detection, temperature, beam size, etc.), explore different ways of parsing the JSON output (e.g., subtitling), and see how to chunk longer audio files.\n",
        "\n",
        "**Part II: TTS Exercises** – You'll send text to be synthesized, view the resulting WAV file directly in the notebook, preprocess input text, visualize the audio waveform, and apply audio modifications.\n",
        "\n",
        "Each exercise includes a brief description and code. The goal of these exercises is to experiment and have fun, but more advanced users are welcome to try out their own code.\n",
        "\n",
        "Happy experimenting!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we begin, please run the following code. This will style the rest of the document such that it will be easier to read the code output."
      ],
      "metadata": {
        "id": "adpz771QuM0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "cwMNtmcwuIRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, run the following code to download all the audio you will need for testing."
      ],
      "metadata": {
        "id": "VSzO6ZUbKIiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download all example audio files\n",
        "for i in range(1, 13):  # 1 to 12\n",
        "    wav_url = f\"https://raw.githubusercontent.com/ZLSLU/EoAI/main/EoAI_sample_{i}.wav\"\n",
        "    txt_url = f\"https://raw.githubusercontent.com/ZLSLU/EoAI/main/EoAI_sample_{i}.txt\"\n",
        "    !wget \"{wav_url}\" -O \"EoAI_sample_{i}.wav\"\n",
        "    !wget \"{txt_url}\" -O \"EoAI_sample_{i}.txt\"\n",
        "\n",
        "!wget \"https://raw.githubusercontent.com/ZLSLU/EoAI/main/EoAI_longform_1.mp3\" -O \"EoAI_longform_1.mp3\"\n",
        "!wget \"https://raw.githubusercontent.com/ZLSLU/EoAI/main/EoAI_longform_1.txt\" -O \"EoAI_longform_1.txt\"\n",
        "!wget \"https://raw.githubusercontent.com/ZLSLU/EoAI/main/EoAI_longform_2.mp3\" -O \"EoAI_longform_2.mp3\"\n",
        "!wget \"https://raw.githubusercontent.com/ZLSLU/EoAI/main/EoAI_longform_2.txt\" -O \"EoAI_longform_2.txt\""
      ],
      "metadata": {
        "id": "A6RTgAUwKCs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0i4uH6zlhYT"
      },
      "source": [
        "## Part I: Speech-to-Text (STT) Exercises\n",
        "\n",
        "In this section, you'll experiment with the STT endpoint by sending an audio file along with various parameters to see how they affect the transcription."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEGC5CpKlhYU"
      },
      "source": [
        "### Exercise STT-1: Test Health Endpoint\n",
        "\n",
        "Verify that the API is accessible by checking the health endpoint. If you are experiencing issues or long delays, run this code to make sure the API is functioning. If the health check fails, please inform the instructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h715tdz2lhYU"
      },
      "outputs": [],
      "source": [
        "# exercise stt-1: test health endpoint\n",
        "import requests\n",
        "\n",
        "response = requests.get(\"http://34.90.113.55:5000/api/health\")\n",
        "print(\"status code:\", response.status_code)\n",
        "print(\"json response:\", response.json())\n",
        "\n",
        "# curl equivalent:\n",
        "# curl http://34.90.113.55:5000/api/health"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV8PZHEglhYV"
      },
      "source": [
        "### Exercise STT-2: Basic File Upload for STT Transcription\n",
        "\n",
        "Send an audio file to the STT endpoint. This exercise uses the `transcribed` model with the language set to Luxembourgish (`lb`). If many users are using this model, you may change to the `timestamped` model instead, which is a second version of the same model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3yNUPaplhYV"
      },
      "outputs": [],
      "source": [
        "# exercise stt-2: basic file upload for stt\n",
        "audio_file_path = \"EoAI_sample_1.wav\"\n",
        "\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\"}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"uploaded stt response status:\", response.status_code)\n",
        "    print(\"uploaded stt response json:\", response.json())\n",
        "\n",
        "# curl equivalent:\n",
        "# curl -F \"file=@EoAI_sample_1.wav\" -F \"model_type=transcribed\" -F \"model_params={\\\"language\\\":\\\"lb\\\"}\" http://34.90.113.55:5000/api/stt/transcribe"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also just output the text, using the same code with a minor change:"
      ],
      "metadata": {
        "id": "uQcus1Qt7r2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise stt-2: basic file upload for stt\n",
        "audio_file_path = \"EoAI_longform2.mp3\"\n",
        "\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\"}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    response_json = response.json()\n",
        "    transcribed_text = response_json.get(\"result\", {}).get(\"text\", \"No transcription found\")\n",
        "    print(transcribed_text)\n"
      ],
      "metadata": {
        "id": "Sno_MQET7rjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmlJxoqClhYW"
      },
      "source": [
        "### Exercise STT-3: Test VAD Parameter\n",
        "\n",
        "Enable Voice Activity Detection (VAD) to remove non-speech segments. Here, we test it by setting `vad` to `true` (which by default uses Silero VAD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BK8bOZ1lhYW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# set the vad setting: change to True or False as desired\n",
        "vad_setting = True\n",
        "\n",
        "# open the audio file in binary mode\n",
        "audio_file_path = \"EoAI_long.mp3\"\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    # build the model_params dictionary and convert it to a json string\n",
        "    model_params = {\"language\": \"lb\", \"vad\": vad_setting}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": json.dumps(model_params)\n",
        "    }\n",
        "    # send the file to the stt endpoint\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"vad\", vad_setting, \"response status:\", response.status_code)\n",
        "    response_json = response.json()\n",
        "    print(\"vad\", vad_setting, \"response json:\", response_json)\n",
        "\n",
        "# if the response is successful and contains result data, compute and print statistics\n",
        "if response.status_code == 200 and \"result\" in response_json:\n",
        "    result = response_json[\"result\"]\n",
        "    segments = result.get(\"segments\", [])\n",
        "    num_segments = len(segments)\n",
        "    # compute average confidence across segments (if available)\n",
        "    confidences = [seg.get(\"confidence\", 0) for seg in segments]\n",
        "    avg_confidence = sum(confidences) / len(confidences) if confidences else 0\n",
        "    # get the overall transcription text from the result\n",
        "    transcription_text = result.get(\"text\", \"\")\n",
        "\n",
        "    print(\"\\nstatistics:\")\n",
        "    print(\"number of segments:\", num_segments)\n",
        "    print(\"average confidence:\", round(avg_confidence, 3))\n",
        "    print(\"transcription text snippet:\", transcription_text[:400], \"...\")\n",
        "else:\n",
        "    print(\"error: no valid result in response\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAh6oXOflhYW"
      },
      "source": [
        "### Exercise STT-4: Test detect_disfluencies Parameter\n",
        "\n",
        "Enable disfluency detection to mark hesitations or filler words in the transcription. This can help in applications where you want to highlight or remove disfluencies (words like \"umm\", \"ëëë\", etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHgEK1XzlhYW"
      },
      "outputs": [],
      "source": [
        "# exercise stt-4: test detect_disfluencies parameter\n",
        "audio_file_path = \"tts_output_eee.wav\"\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\", \"detect_disfluencies\": true}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"detect_disfluencies response status:\", response.status_code)\n",
        "    print(\"detect_disfluencies response json:\", response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKIV-_4YlhYX"
      },
      "source": [
        "### Exercise STT-5: Test trust_whisper_timestamps Parameter\n",
        "\n",
        "Whisper generates timestamps (start & end times) for words as it transcribes speech. However, these timestamps aren’t always perfect—sometimes they might:\n",
        "\n",
        "-\tBe slightly off (e.g., a word starts earlier or later than expected).\n",
        "-\tOverlap or be out of order in fast speech.\n",
        "-\tHave gaps between words where speech is actually continuous.\n",
        "\n",
        "This setting determines whether to strictly follow Whisper’s timestamps or allow some adjustments.\n",
        "\n",
        "Toggle the use of Whisper's timestamps. Here we test by setting `trust_whisper_timestamps` to `false`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpzq-Be3lhYX"
      },
      "outputs": [],
      "source": [
        "# exercise stt-5: test trust_whisper_timestamps parameter set to false\n",
        "audio_file_path = \"EoAI_sample_5.wav\"\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\", \"trust_whisper_timestamps\": false}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"trust_whisper_timestamps false response status:\", response.status_code)\n",
        "    print(\"trust_whisper_timestamps false response json:\", response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following script allows you to compare the same file with the settings on true or false, you just need to change the filename."
      ],
      "metadata": {
        "id": "nzqVzxyR1cZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "def call_stt(trust_flag):\n",
        "    audio_file_path = \"EoAI_long.mp3\"  # update with the file you want to compare\n",
        "    with open(audio_file_path, \"rb\") as f:\n",
        "        files = {\"file\": f}\n",
        "        model_params = {\"language\": \"lb\", \"trust_whisper_timestamps\": trust_flag}\n",
        "        data = {\n",
        "            \"model_type\": \"timestamped\",\n",
        "            \"model_params\": json.dumps(model_params)\n",
        "        }\n",
        "        # send the request to the stt endpoint\n",
        "        response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "        return response.json()\n",
        "\n",
        "# call the stt endpoint with trust_whisper_timestamps true and false\n",
        "result_true = call_stt(True)\n",
        "result_false = call_stt(False)\n",
        "\n",
        "# extract the segments from both responses (each segment should include \"start\" and \"end\")\n",
        "segments_true = result_true.get(\"result\", {}).get(\"segments\", [])\n",
        "segments_false = result_false.get(\"result\", {}).get(\"segments\", [])\n",
        "\n",
        "# print a header for clarity\n",
        "print(\"comparison of segment timestamps for trust_whisper_timestamps settings:\")\n",
        "print(\"{:<10} {:<30} {:<30}\".format(\"segment\", \"trust true (start --> end)\", \"trust false (start --> end)\"))\n",
        "\n",
        "# determine the maximum number of segments from either response\n",
        "max_segments = max(len(segments_true), len(segments_false))\n",
        "\n",
        "for i in range(max_segments):\n",
        "    # if a segment exists for the 'true' setting, extract its start and end times; otherwise, set as N/A\n",
        "    if i < len(segments_true):\n",
        "        seg_true = segments_true[i]\n",
        "        start_true = seg_true.get(\"start\", 0)\n",
        "        end_true = seg_true.get(\"end\", 0)\n",
        "        ts_true = f\"{start_true:.2f} --> {end_true:.2f}\"\n",
        "    else:\n",
        "        ts_true = \"N/A\"\n",
        "\n",
        "    # do the same for the 'false' setting\n",
        "    if i < len(segments_false):\n",
        "        seg_false = segments_false[i]\n",
        "        start_false = seg_false.get(\"start\", 0)\n",
        "        end_false = seg_false.get(\"end\", 0)\n",
        "        ts_false = f\"{start_false:.2f} --> {end_false:.2f}\"\n",
        "    else:\n",
        "        ts_false = \"N/A\"\n",
        "\n",
        "    # print the segment number and both timestamp ranges side by side\n",
        "    print(\"{:<10} {:<30} {:<30}\".format(i+1, ts_true, ts_false))"
      ],
      "metadata": {
        "id": "v6yo6PGe3AFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awSa75gelhYX"
      },
      "source": [
        "### Exercise STT-6: Test compute_word_confidence Parameter\n",
        "\n",
        "Enable the computation of word confidence scores. This parameter allows you to see how confident the model is about each word in the transcription."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Mfhf6jHlhYX"
      },
      "outputs": [],
      "source": [
        "# exercise stt-6: test compute_word_confidence parameter\n",
        "audio_file_path = \"EoAI_sample_10.wav\"\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\", \"compute_word_confidence\": true}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"compute_word_confidence response status:\", response.status_code)\n",
        "    print(\"compute_word_confidence response json:\", response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9WCyw-GlhYX"
      },
      "source": [
        "### Exercise STT-7: Test refine_whisper_precision Parameter\n",
        "\n",
        "Adjust the refinement of Whisper’s segment positions. Here we set `refine_whisper_precision` to `0.4` seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE7fGPxZlhYX"
      },
      "outputs": [],
      "source": [
        "# exercise stt-7: test refine_whisper_precision parameter (set to 0.4)\n",
        "audio_file_path = \"EoAI_longform_2.mp3\"\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\", \"refine_whisper_precision\": 0.4}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"refine_whisper_precision response status:\", response.status_code)\n",
        "    print(\"refine_whisper_precision response json:\", response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try comparing two different values for the same file."
      ],
      "metadata": {
        "id": "sFsLpBXq9q7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "def call_stt(precision_value):\n",
        "    \"\"\"Call the STT API with a specific refine_whisper_precision value\"\"\"\n",
        "    audio_file_path = \"EoAI_longform2.mp3\"\n",
        "\n",
        "    with open(audio_file_path, \"rb\") as f:\n",
        "        files = {\"file\": f}\n",
        "        model_params = {\"language\": \"lb\", \"refine_whisper_precision\": precision_value}\n",
        "        data = {\n",
        "            \"model_type\": \"timestamped\",\n",
        "            \"model_params\": json.dumps(model_params)\n",
        "        }\n",
        "\n",
        "        # Send the request to the STT API\n",
        "        response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "        return response.json()\n",
        "\n",
        "# Call the STT API with two different refine_whisper_precision values\n",
        "precision_1 = 0.2\n",
        "precision_2 = 1.0\n",
        "\n",
        "result_1 = call_stt(precision_1)\n",
        "result_2 = call_stt(precision_2)\n",
        "\n",
        "# Extract the transcribed segments\n",
        "segments_1 = result_1.get(\"result\", {}).get(\"segments\", [])\n",
        "segments_2 = result_2.get(\"result\", {}).get(\"segments\", [])\n",
        "\n",
        "# Print comparison table header\n",
        "print(f\"\\nComparison of Segment Timestamps for refine_whisper_precision = {precision_1} vs {precision_2}\")\n",
        "print(\"{:<10} {:<30} {:<30}\".format(\"Segment\", f\"Precision {precision_1} (start --> end)\", f\"Precision {precision_2} (start --> end)\"))\n",
        "\n",
        "# Determine the maximum number of segments\n",
        "max_segments = max(len(segments_1), len(segments_2))\n",
        "\n",
        "for i in range(max_segments):\n",
        "    # If a segment exists for precision_1, extract its start and end times; otherwise, set as N/A\n",
        "    if i < len(segments_1):\n",
        "        seg_1 = segments_1[i]\n",
        "        start_1 = seg_1.get(\"start\", 0)\n",
        "        end_1 = seg_1.get(\"end\", 0)\n",
        "        ts_1 = f\"{start_1:.2f} --> {end_1:.2f}\"\n",
        "    else:\n",
        "        ts_1 = \"N/A\"\n",
        "\n",
        "    # Do the same for precision_2\n",
        "    if i < len(segments_2):\n",
        "        seg_2 = segments_2[i]\n",
        "        start_2 = seg_2.get(\"start\", 0)\n",
        "        end_2 = seg_2.get(\"end\", 0)\n",
        "        ts_2 = f\"{start_2:.2f} --> {end_2:.2f}\"\n",
        "    else:\n",
        "        ts_2 = \"N/A\"\n",
        "\n",
        "    # Print the segment number and both timestamp ranges side by side\n",
        "    print(\"{:<10} {:<30} {:<30}\".format(i+1, ts_1, ts_2))"
      ],
      "metadata": {
        "id": "nrkOd_qJ-UjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svuVMk6UlhYY"
      },
      "source": [
        "### Exercise STT-8: Test Temperature Parameter\n",
        "\n",
        "Experiment with the sampling temperature. Here we pass a list of fallback temperatures to see if it influences the transcription output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1HC7BbslhYY"
      },
      "outputs": [],
      "source": [
        "# exercise stt-8: test temperature parameter (using a list of fallback temperatures)\n",
        "audio_file_path = \"EoAI_sample_12.wav\"\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\", \"temperature\": 1}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"temperature parameter response status:\", response.status_code)\n",
        "    response_json = response.json()\n",
        "    transcribed_text = response_json.get(\"result\", {}).get(\"text\", \"No transcription found\")\n",
        "    print(transcribed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you would like to compare the difference between your temperature settings and the official transcription, run the code below with the filename you want to compare."
      ],
      "metadata": {
        "id": "LDra3YpIBR_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "# Define the audio file\n",
        "audio_file_path = \"EoAI_sample_12.wav\"\n",
        "text_file_path = audio_file_path.replace(\".wav\", \".txt\")  # Matching text file\n",
        "\n",
        "# Read the reference text file (if it exists)\n",
        "if os.path.exists(text_file_path):\n",
        "    with open(text_file_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
        "        reference_text = txt_file.read().strip()\n",
        "else:\n",
        "    reference_text = \"Reference text not found.\"\n",
        "\n",
        "# Call the STT API\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\", \"temperature\": 1.5}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "\n",
        "# Check API response\n",
        "print(\"\\nTemperature Parameter Response Status:\", response.status_code)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    response_json = response.json()\n",
        "    transcribed_text = response_json.get(\"result\", {}).get(\"text\", \"No transcription found\")\n",
        "else:\n",
        "    transcribed_text = \"Error: Failed to transcribe audio.\"\n",
        "\n",
        "# Print Comparison\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📜 Reference Text (from .txt file):\\n\")\n",
        "print(reference_text)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📝 Transcribed Text (from STT API):\\n\")\n",
        "print(transcribed_text)\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "id": "cg-DhqiQBcVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-_25JGslhYY"
      },
      "source": [
        "### Exercise STT-9: Test Beam Size and Best Of Parameters\n",
        "\n",
        "Adjust beam search parameters (`beam_size` and `best_of`) to see how they affect transcription quality and speed.\n",
        "\n",
        "`beam_size` will generate X transcription candidates, and pick the most optimal one. This happens during one single run of the model. `best_of` will run the model X times independently, and choose the optimal candidate in the end.\n",
        "\n",
        "In the example here, `beam_size` is set and `best_of` is not. This is simply to save on GPU resources, but both can be tested independently, though not together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjnh6WgklhYY"
      },
      "outputs": [],
      "source": [
        "# exercise stt-9: test beam_size and best_of parameters\n",
        "audio_file_path = \"EoAI_sample_6.wav\"\n",
        "with open(audio_file_path, \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\", \"beam_size\": 5}'\n",
        "    }\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    print(\"beam_size and best_of response status:\", response.status_code)\n",
        "    print(\"beam_size and best_of response json:\", response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3Oy6e4BlhYY"
      },
      "source": [
        "### Exercise STT-10: Convert STT JSON Output to SRT Format\n",
        "\n",
        "This script converts a JSON transcription response into SRT subtitle format, ensuring timestamps and subtitles are formatted correctly. It first converts time in seconds into SRT timecode format (hh:mm:ss,ms), then processes transcription segments, splitting long ones into smaller chunks without cutting words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIw_9IxalhYY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def sec_to_time(seconds):\n",
        "    # convert seconds to srt time format: hh:mm:ss,ms\n",
        "    hours = math.floor(seconds / 3600)\n",
        "    minutes = math.floor((seconds % 3600) / 60)\n",
        "    secs = seconds % 60\n",
        "    millis = int((secs - int(secs)) * 1000)\n",
        "    return f\"{hours:02}:{minutes:02}:{int(secs):02},{millis:03}\"\n",
        "\n",
        "def json_to_srt(result_json):\n",
        "    # get the list of transcription segments from the json\n",
        "    segments = result_json.get(\"result\", {}).get(\"segments\", [])\n",
        "    srt_output = \"\"\n",
        "    srt_counter = 1  # subtitle numbering starts at 1\n",
        "    max_chunk_duration = 5.0  # maximum duration per subtitle in seconds\n",
        "\n",
        "    for seg in segments:\n",
        "        # retrieve segment start and end times and the overall segment text\n",
        "        seg_start = seg.get(\"start\", 0)\n",
        "        seg_end = seg.get(\"end\", 0)\n",
        "        seg_text = seg.get(\"text\", \"\").strip()\n",
        "        duration = seg_end - seg_start\n",
        "\n",
        "        # if the segment is short (<= max duration) or no word-level info is available,\n",
        "        # output the segment as a single subtitle\n",
        "        if duration <= max_chunk_duration or \"words\" not in seg:\n",
        "            srt_output += f\"{srt_counter}\\n{sec_to_time(seg_start)} --> {sec_to_time(seg_end)}\\n{seg_text}\\n\\n\"\n",
        "            srt_counter += 1\n",
        "        else:\n",
        "            # if the segment is long and word-level timestamps exist, we split it without cutting words\n",
        "            words = seg.get(\"words\", [])\n",
        "            if not words:\n",
        "                # fallback if words list is empty\n",
        "                srt_output += f\"{srt_counter}\\n{sec_to_time(seg_start)} --> {sec_to_time(seg_end)}\\n{seg_text}\\n\\n\"\n",
        "                srt_counter += 1\n",
        "                continue\n",
        "\n",
        "            # start the first subtitle chunk at the segment's start time\n",
        "            current_chunk_start = seg_start\n",
        "            current_chunk_words = []  # list to collect words for the current subtitle\n",
        "\n",
        "            for word in words:\n",
        "                # get the text of the word (using the 'text' key, as provided in your json)\n",
        "                word_text = word.get(\"text\", \"\")\n",
        "                # get the start time of the word; if missing, default to current chunk start\n",
        "                word_start = word.get(\"start\", current_chunk_start)\n",
        "\n",
        "                # if adding this word would exceed our maximum subtitle duration,\n",
        "                # finish the current subtitle chunk before this word starts\n",
        "                if (word_start - current_chunk_start) > max_chunk_duration:\n",
        "                    # join the collected words into a string for the current subtitle chunk\n",
        "                    chunk_text = \" \".join(current_chunk_words)\n",
        "                    # set the current chunk's end time to the start time of this word\n",
        "                    current_chunk_end = word_start\n",
        "                    srt_output += f\"{srt_counter}\\n{sec_to_time(current_chunk_start)} --> {sec_to_time(current_chunk_end)}\\n{chunk_text}\\n\\n\"\n",
        "                    srt_counter += 1\n",
        "                    # start a new chunk beginning with the current word\n",
        "                    current_chunk_start = word_start\n",
        "                    current_chunk_words = [word_text]\n",
        "                else:\n",
        "                    # if within the allowed duration, add the word to the current chunk\n",
        "                    current_chunk_words.append(word_text)\n",
        "            # output any remaining words as the final subtitle chunk for this segment\n",
        "            if current_chunk_words:\n",
        "                srt_output += f\"{srt_counter}\\n{sec_to_time(current_chunk_start)} --> {sec_to_time(seg_end)}\\n{' '.join(current_chunk_words)}\\n\\n\"\n",
        "                srt_counter += 1\n",
        "\n",
        "    return srt_output\n",
        "\n",
        "\n",
        "result_json = response.json()  # using the latest stt response\n",
        "srt_text = json_to_srt(result_json)\n",
        "print(\"srt output:\\n\", srt_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR4RQAIflhYY"
      },
      "source": [
        "### Exercise STT-11: Chunking Long Audio Files\n",
        "\n",
        "This script splits a long audio file into smaller 15-second chunks using pydub, then sends each chunk separately to an STT (speech-to-text) API for transcription. Each chunk’s transcribed segments are adjusted to reflect their actual position in the original audio, ensuring correct timestamps. Finally, all segments are combined into a single transcription, reconstructing the full text from the individual STT results.\n",
        "\n",
        "This script is also more advanced, but you can change the following variables to test it out:\n",
        "\n",
        "`file_path` : the file you want to transcribe\n",
        "`chunk_size` : how many seconds long each chunk should be"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzl6kJDKlhYY"
      },
      "outputs": [],
      "source": [
        "# install pydub\n",
        "!pip install pydub\n",
        "\n",
        "import math\n",
        "from pydub import AudioSegment\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "def chunk_audio_in_memory(file_path, chunk_duration):\n",
        "    \"\"\"\n",
        "    read an audio file and split it into chunks of chunk_duration seconds.\n",
        "    returns a list of tuples: (audio_chunk, chunk_start_time_in_seconds).\n",
        "    \"\"\"\n",
        "    # load the entire audio file into memory\n",
        "    audio = AudioSegment.from_file(file_path)\n",
        "    duration_ms = len(audio)  # duration in milliseconds\n",
        "    chunks = []\n",
        "    # iterate through the audio in increments of chunk_duration (converted to ms)\n",
        "    for start in range(0, duration_ms, int(chunk_duration * 1000)):\n",
        "        end = min(start + int(chunk_duration * 1000), duration_ms)\n",
        "        chunk = audio[start:end]\n",
        "        # append the chunk along with its start time (in seconds)\n",
        "        chunks.append((chunk, start / 1000.0))\n",
        "    return chunks\n",
        "\n",
        "def process_chunk(chunk, chunk_start, model_type=\"transcribed\", model_params='{\"language\": \"lb\"}'):\n",
        "    \"\"\"\n",
        "    export an audio chunk to a temporary in-memory wav file,\n",
        "    send it to the stt endpoint, and adjust the timestamps of the returned segments.\n",
        "    \"\"\"\n",
        "    # create a bytes buffer to hold the wav file data\n",
        "    chunk_io = BytesIO()\n",
        "    # export the chunk in wav format into the bytes buffer\n",
        "    chunk.export(chunk_io, format=\"wav\")\n",
        "    # reset pointer to the beginning of the buffer\n",
        "    chunk_io.seek(0)\n",
        "\n",
        "    # prepare the file data for multipart/form-data\n",
        "    files = {\"file\": (\"chunk.wav\", chunk_io, \"audio/wav\")}\n",
        "    data = {\n",
        "        \"model_type\": model_type,\n",
        "        \"model_params\": model_params\n",
        "    }\n",
        "    # send the chunk to the stt endpoint (adjust the url if needed)\n",
        "    response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    result_json = response.json()\n",
        "\n",
        "    # adjust the timing in each segment by adding the chunk's start time\n",
        "    segments = result_json.get(\"result\", {}).get(\"segments\", [])\n",
        "    for seg in segments:\n",
        "        seg[\"start\"] += chunk_start\n",
        "        seg[\"end\"] += chunk_start\n",
        "        # adjust word-level timestamps if available\n",
        "        if \"words\" in seg:\n",
        "            for word in seg[\"words\"]:\n",
        "                word[\"start\"] += chunk_start\n",
        "                word[\"end\"] += chunk_start\n",
        "    return result_json\n",
        "\n",
        "# main process: chunk the file, process each chunk, and combine the results\n",
        "file_path = \"EoAI_longform2.mp3\"  # update with your audio file path\n",
        "chunk_duration = 15  # seconds per chunk\n",
        "\n",
        "# get list of (chunk, start_time) tuples\n",
        "chunks = chunk_audio_in_memory(file_path, chunk_duration)\n",
        "print(\"number of chunks:\", len(chunks))\n",
        "\n",
        "all_segments = []  # list to hold all transcription segments\n",
        "\n",
        "# process each chunk one by one\n",
        "for chunk, start_time in chunks:\n",
        "    print(\"processing chunk starting at\", start_time, \"seconds\")\n",
        "    result_json = process_chunk(chunk, start_time)\n",
        "    segments = result_json.get(\"result\", {}).get(\"segments\", [])\n",
        "    all_segments.extend(segments)\n",
        "\n",
        "# combine all segments into one final result\n",
        "final_text = \" \".join(seg.get(\"text\", \"\") for seg in all_segments)\n",
        "final_result = {\n",
        "    \"result\": {\n",
        "        \"segments\": all_segments,\n",
        "        \"text\": final_text\n",
        "    },\n",
        "    \"status\": \"success\"\n",
        "}\n",
        "\n",
        "print(\"final combined transcription:\")\n",
        "print(final_result[\"result\"][\"text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWOCWMuMlhYZ"
      },
      "source": [
        "### Exercise STT-12: Advanced JSON Parsing – Extract Word-level Timestamps\n",
        "\n",
        "Extract word-level timestamps from the STT JSON output and display them in a table using Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qfq9j_4glhYa"
      },
      "outputs": [],
      "source": [
        "# exercise stt-12: extract word-level timestamps and display in a table\n",
        "import pandas as pd\n",
        "\n",
        "def extract_word_timestamps(result_json):\n",
        "    # assume each segment has a 'words' key with a list of dicts with 'word', 'start', 'end'\n",
        "    segments = result_json.get(\"result\", {}).get(\"segments\", [])\n",
        "    rows = []\n",
        "    for seg in segments:\n",
        "        words = seg.get(\"words\", [])\n",
        "        for w in words:\n",
        "            rows.append({\n",
        "                \"word\": w.get(\"word\", \"\"),\n",
        "                \"start\": w.get(\"start\", 0),\n",
        "                \"end\": w.get(\"end\", 0)\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# demo extraction using latest stt response\n",
        "df = extract_word_timestamps(result_json)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J87mrgN8lhYa"
      },
      "source": [
        "## Part II: Text-to-Speech (TTS) Exercises\n",
        "\n",
        "In this section, you'll experiment with the TTS endpoint. You will send text to synthesize audio, display the resulting WAV file directly in the notebook, preprocess text, visualize waveforms, and apply some audio modifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy-dilFelhYa"
      },
      "source": [
        "### Exercise TTS-1: Basic TTS Request\n",
        "\n",
        "Send a simple text to the TTS endpoint and save the resulting WAV file. Then, display the audio directly in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qehHElXlhYa"
      },
      "outputs": [],
      "source": [
        "# exercise tts-1: basic tts request\n",
        "tts_payload = {\"text\": \"Gudde Moien, hei ass e Beispill vun der Sproochsynthees an der Lëtzebuerger Sprooch.\"}\n",
        "response = requests.post(\"http://34.90.113.55:5000/api/tts/synthesize\", json=tts_payload, timeout=120)\n",
        "print(\"tts response status:\", response.status_code)\n",
        "\n",
        "with open(\"tts_output.wav\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "print(\"saved tts_output.wav locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXK1WeBwlhYa"
      },
      "outputs": [],
      "source": [
        "# exercise tts-1: display the tts output audio\n",
        "from IPython.display import Audio\n",
        "Audio(\"tts_output.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipsnR0dlhYa"
      },
      "source": [
        "### Exercise TTS-2: Preprocess Text for TTS\n",
        "\n",
        "Without text preprocessing, the model will not provide optimal results. In the previous example, the pronunciation was strange. This is because all text must be set to lowercase.\n",
        "\n",
        "Preprocess the input text (e.g., convert to lowercase and trim extra whitespace) before sending it to the TTS endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpBvCAJVlhYa"
      },
      "outputs": [],
      "source": [
        "# exercise tts-2: preprocess text before tts\n",
        "raw_text = \"Gudde Moien, hei ass e Beispill vun der Sproochsynthees an der Lëtzebuerger Sprooch.\"\n",
        "processed_text = raw_text.lower().strip()\n",
        "print(\"processed text:\", processed_text)\n",
        "\n",
        "tts_payload = {\"text\": processed_text}\n",
        "response = requests.post(\"http://34.90.113.55:5000/api/tts/synthesize\", json=tts_payload, timeout=120)\n",
        "\n",
        "with open(\"tts_output.wav\", \"wb\") as f:\n",
        "    f.write(response.content)\n",
        "print(\"saved tts_output.wav locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VkuAbyvlhYa"
      },
      "outputs": [],
      "source": [
        "# exercise tts-2: display processed tts audio\n",
        "Audio(\"tts_output.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0SbyaA0lhYa"
      },
      "source": [
        "### Exercise TTS-3: Visualize TTS Waveform\n",
        "\n",
        "Load the synthesized audio and plot its waveform using matplotlib.\n",
        "\n",
        "- Helps analyze TTS quality by visualizing the waveform for distortions, unnatural pauses, or clipping.\n",
        "- Allows comparison between different parameters to check for smoothness, loudness, and structure.\n",
        "- Detects silent gaps or glitches in synthesis that might not be noticeable just by listening.\n",
        "- Useful for debugging issues related to volume, pitch variations, or incorrect synthesis timing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9liz7BTOlhYa"
      },
      "outputs": [],
      "source": [
        "# exercise tts-3: visualize the waveform of tts output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "\n",
        "data, samplerate = sf.read(\"tts_output.wav\")\n",
        "times = np.linspace(0, len(data)/samplerate, num=len(data))\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(times, data)\n",
        "plt.title(\"Waveform of TTS Output\")\n",
        "plt.xlabel(\"Time (s)\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yabFi3UtlhYa"
      },
      "source": [
        "### Exercise TTS-4: Modify TTS Output with an Audio Filter\n",
        "\n",
        "Use `pydub` to apply an audio filter to the TTS output—for example, increasing the playback speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiqdJHb6lhYa"
      },
      "outputs": [],
      "source": [
        "# exercise tts-4: modify tts output - change speed using pydub\n",
        "from pydub import AudioSegment\n",
        "\n",
        "tts_audio = AudioSegment.from_wav(\"tts_output.wav\")\n",
        "# increase speed by 1.5x\n",
        "faster_audio = tts_audio.speedup(playback_speed=1.5)\n",
        "faster_audio.export(\"tts_faster.wav\", format=\"wav\")\n",
        "print(\"exported tts_faster.wav\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2L8Y4MklhYa"
      },
      "outputs": [],
      "source": [
        "# exercise tts-4: display modified tts audio\n",
        "Audio(\"tts_faster.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVr0XZrvlhYa"
      },
      "source": [
        "### Exercise TTS-5: Chain STT and TTS\n",
        "\n",
        "Transcribe an audio file using the STT endpoint and then synthesize the resulting text using the TTS endpoint. This demonstrates a full processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKjMYTx1lhYa"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# exercise tts-5: chain stt and tts\n",
        "with open(\"EoAI_sample_8.wav\", \"rb\") as f:\n",
        "    files = {\"file\": f}\n",
        "    data = {\n",
        "        \"model_type\": \"transcribed\",\n",
        "        \"model_params\": '{\"language\": \"lb\"}'\n",
        "    }\n",
        "    stt_response = requests.post(\"http://34.90.113.55:5000/api/stt/transcribe\", files=files, data=data, timeout=600)\n",
        "    stt_result = stt_response.json()\n",
        "    transcription_text = stt_result.get(\"result\", {}).get(\"text\", \"\")\n",
        "    print(\"transcription:\", transcription_text)\n",
        "\n",
        "tts_payload = {\"text\": transcription_text.lower().strip()}\n",
        "tts_response = requests.post(\"http://34.90.113.55:5000/api/tts/synthesize\", json=tts_payload, timeout=120)\n",
        "with open(\"chain_tts.wav\", \"wb\") as f:\n",
        "    f.write(tts_response.content)\n",
        "print(\"saved chain_tts.wav locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mTNr2gKlhYc"
      },
      "outputs": [],
      "source": [
        "# exercise tts-5: display chained tts audio\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio(\"chain_tts.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZeUEAMylhYc"
      },
      "source": [
        "### Exercise TTS-6: Advanced TTS Modification – Adjust Volume\n",
        "\n",
        "Use `pydub` to modify the volume of the synthesized audio (e.g. increase by 6 dB) and then play the modified output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAAzphKGlhYc"
      },
      "outputs": [],
      "source": [
        "# exercise tts-7: advanced tts modification - adjust volume using pydub\n",
        "\n",
        "tts_audio = AudioSegment.from_wav(\"tts_output.wav\")\n",
        "# increase volume by 6 dB\n",
        "louder_audio = tts_audio + 6\n",
        "louder_audio.export(\"tts_louder.wav\", format=\"wav\")\n",
        "print(\"exported tts_louder.wav with increased volume\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAVWxkGGlhYc"
      },
      "outputs": [],
      "source": [
        "# exercise tts-7: display advanced tts modified audio\n",
        "from IPython.display import Audio\n",
        "\n",
        "Audio(\"tts_louder.wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "NdUkgVO3jnSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from IPython.display import display, HTML, Audio\n",
        "\n",
        "# get a list of audio files with wav and mp3 extensions\n",
        "wav_files = glob.glob(\"*.wav\")\n",
        "mp3_files = glob.glob(\"*.mp3\")\n",
        "audio_files = sorted(wav_files + mp3_files)  # sort files alphabetically\n",
        "\n",
        "rows = []  # list to hold (filename, audio display html, transcription text) tuples\n",
        "\n",
        "for audio_file in audio_files:\n",
        "    # get the base filename (without extension)\n",
        "    base = audio_file.rsplit(\".\", 1)[0]\n",
        "    txt_file = base + \".txt\"  # expected corresponding transcription file\n",
        "    try:\n",
        "        with open(txt_file, \"r\") as f:\n",
        "            transcription = f.read().strip()\n",
        "    except FileNotFoundError:\n",
        "        transcription = \"no transcription found\"\n",
        "\n",
        "    # create an embedded audio player using IPython.display.Audio\n",
        "    audio_display = Audio(audio_file, embed=True)._repr_html_()\n",
        "    rows.append((audio_file, audio_display, transcription))\n",
        "\n",
        "# sort rows alphabetically by filename\n",
        "rows.sort()\n",
        "\n",
        "# build an html table to display the filenames, audio players, and corresponding transcriptions\n",
        "table_html = \"<table border='1' style='border-collapse: collapse; width: 100%;'>\"\n",
        "table_html += \"<tr><th>filename</th><th>audio</th><th>reference transcription</th></tr>\"\n",
        "for filename, audio_disp, trans in rows:\n",
        "    table_html += f\"<tr><td>{filename}</td><td>{audio_disp}</td><td>{trans}</td></tr>\"\n",
        "table_html += \"</table>\"\n",
        "\n",
        "display(HTML(table_html))"
      ],
      "metadata": {
        "id": "FqxES0-Qr2MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_error_rate(reference, hypothesis):\n",
        "    # split both strings into lists of words (converted to lowercase to normalize)\n",
        "    ref_words = reference.lower().split()\n",
        "    hyp_words = hypothesis.lower().split()\n",
        "\n",
        "    # create a dp table of size (len(ref_words)+1) x (len(hyp_words)+1)\n",
        "    dp = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
        "\n",
        "    # initialize base cases: distance from empty string\n",
        "    for i in range(len(ref_words) + 1):\n",
        "        dp[i][0] = i  # i deletions\n",
        "    for j in range(len(hyp_words) + 1):\n",
        "        dp[0][j] = j  # j insertions\n",
        "\n",
        "    # fill the dp table using dynamic programming\n",
        "    for i in range(1, len(ref_words) + 1):\n",
        "        for j in range(1, len(hyp_words) + 1):\n",
        "            if ref_words[i - 1] == hyp_words[j - 1]:\n",
        "                dp[i][j] = dp[i - 1][j - 1]  # no error if words are the same\n",
        "            else:\n",
        "                substitution = dp[i - 1][j - 1] + 1\n",
        "                insertion = dp[i][j - 1] + 1\n",
        "                deletion = dp[i - 1][j] + 1\n",
        "                dp[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    edit_distance = dp[len(ref_words)][len(hyp_words)]\n",
        "    # calculate wer as edit distance divided by number of words in the reference\n",
        "    wer = edit_distance / float(len(ref_words)) if ref_words else 0\n",
        "    return wer\n",
        "\n",
        "# read the reference transcription from a text file\n",
        "ref_filename = \"EoAI_long.txt\"  # update with the desired filename\n",
        "with open(ref_filename, \"r\") as f:\n",
        "    reference_transcription = f.read().strip()\n",
        "\n",
        "# assume the model's transcription is obtained from the stt endpoint\n",
        "# for example, using response.json() from an earlier request:\n",
        "model_transcription = response.json().get(\"result\", {}).get(\"text\", \"\")\n",
        "\n",
        "print(\"reference transcription:\")\n",
        "print(reference_transcription)\n",
        "print(\"\\nmodel transcription:\")\n",
        "print(model_transcription)\n",
        "\n",
        "# compute the word error rate between the reference and model transcriptions\n",
        "wer = word_error_rate(reference_transcription, model_transcription)\n",
        "print(\"\\nword error rate (wer):\", round(wer, 3))"
      ],
      "metadata": {
        "id": "RQKcPDj_jpxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3BZdu-ZlhYc"
      },
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "In this notebook you:\n",
        "\n",
        "- Experimented with a range of STT parameters to see how they influence transcription output.\n",
        "- Learned how to parse and convert raw JSON outputs (e.g., to SRT format) and even how to chunk long audio files.\n",
        "- Explored the TTS endpoint by synthesizing speech from text, visualizing waveforms, and applying post-processing filters.\n",
        "\n",
        "Feel free to extend these exercises with your own experiments and modifications. Happy coding!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}